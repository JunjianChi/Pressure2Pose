{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pressure2Pose: Multi-Architecture Training & Comparison\n",
    "\n",
    "This notebook trains and evaluates **5 model architectures** for predicting SMPL body parameters from plantar pressure sequences:\n",
    "\n",
    "| Model | Temporal Method | Description |\n",
    "|-------|----------------|-------------|\n",
    "| A | CNN Baseline | Single frame, no temporal context |\n",
    "| B | CNN + BiGRU | Bidirectional GRU (h=256, 2 layers) |\n",
    "| C | CNN + BiLSTM | Bidirectional LSTM (h=256, 2 layers) |\n",
    "| D | CNN + TCN | Dilated Conv1d (d=1,2,4,8) |\n",
    "| E | CNN + Transformer | TransformerEncoder (d=512, 8 heads, 4 layers) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Available models: ['cnn_baseline', 'cnn_bigru', 'cnn_bilstm', 'cnn_tcn', 'cnn_transformer']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on the path\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from models import build_model, MODEL_REGISTRY\n",
    "from models.pressure_to_smpl import SMPLLoss\n",
    "from datasets import PressureSequenceDataset, create_sequence_dataloaders\n",
    "from utils.metrics import (\n",
    "    compute_mpjpe, compute_pa_mpjpe,\n",
    "    compute_vertex_error, compute_bone_length_error\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Available models: {list(MODEL_REGISTRY.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths ----\n",
    "DATA_ROOT = PROJECT_ROOT / 'data'\n",
    "SMPL_PATH = PROJECT_ROOT / 'smpl_models' / 'SMPL_python_v.1.1.0' / 'SMPL_python_v.1.1.0' / 'smpl' / 'models'\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / 'checkpoints'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Hyper-parameters ----\n",
    "SEQ_LEN     = 32       # Temporal window\n",
    "BATCH_SIZE  = 16\n",
    "EPOCHS      = 80\n",
    "LR          = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE    = 20       # Early stopping patience\n",
    "GRAD_CLIP   = 1.0      # For RNN models\n",
    "WARMUP_EPOCHS = 5      # LR warmup for Transformer\n",
    "\n",
    "# Pressure sensor shape per foot\n",
    "PRESSURE_H, PRESSURE_W = 33, 15\n",
    "\n",
    "# ---- Data split ----\n",
    "# Only walking1_physics.pkl exists. We split it 80/20 by manually\n",
    "# creating train/val loaders from the same sequence.\n",
    "TRAIN_SEQUENCES = ['walking1']\n",
    "VAL_SEQUENCES   = ['walking1']  # Same file; split handled below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\n\n# ---- Build train / val split from a single sequence ----\ncsv_file = DATA_ROOT / 'walking1_cleaned.csv'\npkl_file = DATA_ROOT / 'smpl_params' / 'walking1_physics.pkl'\n\nprint(f'CSV: {csv_file} (exists={csv_file.exists()})')\nprint(f'PKL: {pkl_file} (exists={pkl_file.exists()})')\n\n# Load the full sequence dataset once\nfull_ds = PressureSequenceDataset(\n    [csv_file], [pkl_file],\n    seq_len=SEQ_LEN, stride=1,\n    pressure_shape=(2, PRESSURE_H, PRESSURE_W),\n    normalize=True\n)\n\n# 80/20 split\nn_total = len(full_ds)\nn_train = int(0.8 * n_total)\nn_val   = n_total - n_train\n\ntrain_ds, val_ds = torch.utils.data.random_split(\n    full_ds, [n_train, n_val],\n    generator=torch.Generator().manual_seed(42)\n)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\nval_loader = torch.utils.data.DataLoader(\n    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\nprint(f'\\nTrain: {n_train} windows  |  Val: {n_val} windows')\nprint(f'Train batches: {len(train_loader)}  |  Val batches: {len(val_loader)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize a sample pressure heatmap ----\n",
    "sample = full_ds[0]\n",
    "pressure_seq = sample['pressure']  # (T, 2, H, W)\n",
    "center = SEQ_LEN // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for i, (ax, title) in enumerate(zip(axes, ['Left Foot', 'Right Foot'])):\n",
    "    im = ax.imshow(pressure_seq[center, i].numpy(), cmap='hot',\n",
    "                   interpolation='bilinear', aspect='auto')\n",
    "    ax.set_title(f'{title} Pressure (center frame)', fontsize=12)\n",
    "    ax.set_xlabel('Sensor Column')\n",
    "    ax.set_ylabel('Sensor Row')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Pressure sequence shape: {pressure_seq.shape}')  # (T, 2, H, W)\n",
    "print(f'SMPL target dims: betas={sample[\"betas\"].shape}, '\n",
    "      f'body_pose={sample[\"body_pose\"].shape}, '\n",
    "      f'global_orient={sample[\"global_orient\"].shape}, '\n",
    "      f'transl={sample[\"transl\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Instantiation & Parameter Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    'A: CNN Baseline': {\n",
    "        'model': {'type': 'cnn_baseline', 'feature_dim': 512}\n",
    "    },\n",
    "    'B: CNN + BiGRU': {\n",
    "        'model': {'type': 'cnn_bigru', 'feature_dim': 512,\n",
    "                  'hidden_dim': 256, 'num_layers': 2}\n",
    "    },\n",
    "    'C: CNN + BiLSTM': {\n",
    "        'model': {'type': 'cnn_bilstm', 'feature_dim': 512,\n",
    "                  'hidden_dim': 256, 'num_layers': 2}\n",
    "    },\n",
    "    'D: CNN + TCN': {\n",
    "        'model': {'type': 'cnn_tcn', 'feature_dim': 512,\n",
    "                  'num_blocks': 4, 'kernel_size': 3}\n",
    "    },\n",
    "    'E: CNN + Transformer': {\n",
    "        'model': {'type': 'cnn_transformer', 'feature_dim': 512,\n",
    "                  'nhead': 8, 'num_layers': 4, 'dim_feedforward': 1024}\n",
    "    },\n",
    "}\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "rows = []\n",
    "for name, cfg in MODEL_CONFIGS.items():\n",
    "    m = build_model(cfg)\n",
    "    n = count_params(m)\n",
    "    rows.append({'Model': name, 'Parameters': n, 'Params (M)': f'{n/1e6:.2f}M'})\n",
    "\n",
    "param_df = pd.DataFrame(rows)\n",
    "print(param_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick forward-pass sanity check\n",
    "dummy_seq = torch.randn(2, SEQ_LEN, 2, PRESSURE_H, PRESSURE_W).to(device)\n",
    "\n",
    "for name, cfg in MODEL_CONFIGS.items():\n",
    "    m = build_model(cfg).to(device)\n",
    "    betas, pose, orient, transl = m(dummy_seq)\n",
    "    print(f'{name:25s}  betas={betas.shape}  pose={pose.shape}  '\n",
    "          f'orient={orient.shape}  transl={transl.shape}')\n",
    "    del m\n",
    "\n",
    "print('All models passed forward-pass check.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unified Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smplx\n",
    "\n",
    "# Load SMPL for loss computation (joints / vertices supervision)\n",
    "smpl_layer = smplx.SMPL(\n",
    "    model_path=str(SMPL_PATH),\n",
    "    gender='neutral',\n",
    "    batch_size=1,\n",
    "    create_transl=False\n",
    ").to(device)\n",
    "\n",
    "criterion = SMPLLoss(\n",
    "    lambda_joints=1.0,\n",
    "    lambda_vertices=0.5,\n",
    "    lambda_betas=0.01,\n",
    "    lambda_pose=0.001\n",
    ")\n",
    "\n",
    "print('SMPL layer and loss function ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, config, train_loader, val_loader,\n",
    "                smpl_layer, criterion, device,\n",
    "                epochs=EPOCHS, lr=LR, patience=PATIENCE,\n",
    "                grad_clip=None, warmup_epochs=0):\n",
    "    \"\"\"\n",
    "    Train a single model with early stopping.\n",
    "\n",
    "    Returns:\n",
    "        model: Best model (by val MPJPE)\n",
    "        history: dict with train_loss, val_loss, val_mpjpe lists\n",
    "    \"\"\"\n",
    "    model = build_model(config).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_mpjpe': []}\n",
    "    best_mpjpe = float('inf')\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training: {model_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ---- LR warmup for Transformer ----\n",
    "        if warmup_epochs > 0 and epoch <= warmup_epochs:\n",
    "            warmup_lr = lr * epoch / warmup_epochs\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = warmup_lr\n",
    "\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            pressure = batch['pressure'].to(device)\n",
    "            target_betas = batch['betas'].to(device)\n",
    "            target_pose = batch['body_pose'].to(device)\n",
    "            target_orient = batch['global_orient'].to(device)\n",
    "            target_transl = batch['transl'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            betas, body_pose, global_orient, transl = model(pressure)\n",
    "\n",
    "            # SMPL forward for predicted\n",
    "            bs = pressure.shape[0]\n",
    "            smpl_layer.batch_size = bs\n",
    "            pred_out = smpl_layer(\n",
    "                betas=betas, body_pose=body_pose,\n",
    "                global_orient=global_orient, transl=transl\n",
    "            )\n",
    "\n",
    "            # SMPL forward for target\n",
    "            with torch.no_grad():\n",
    "                tgt_out = smpl_layer(\n",
    "                    betas=target_betas, body_pose=target_pose,\n",
    "                    global_orient=target_orient, transl=target_transl\n",
    "                )\n",
    "\n",
    "            target_dict = {\n",
    "                'joints': tgt_out.joints,\n",
    "                'vertices': tgt_out.vertices\n",
    "            }\n",
    "            _, loss = criterion(\n",
    "                pred_out, target_dict,\n",
    "                pred_params=(betas, body_pose, global_orient, transl)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "\n",
    "        # ---- Validate ----\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        mpjpe_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                pressure = batch['pressure'].to(device)\n",
    "                target_betas = batch['betas'].to(device)\n",
    "                target_pose = batch['body_pose'].to(device)\n",
    "                target_orient = batch['global_orient'].to(device)\n",
    "                target_transl = batch['transl'].to(device)\n",
    "\n",
    "                betas, body_pose, global_orient, transl = model(pressure)\n",
    "                bs = pressure.shape[0]\n",
    "                smpl_layer.batch_size = bs\n",
    "\n",
    "                pred_out = smpl_layer(\n",
    "                    betas=betas, body_pose=body_pose,\n",
    "                    global_orient=global_orient, transl=transl\n",
    "                )\n",
    "                tgt_out = smpl_layer(\n",
    "                    betas=target_betas, body_pose=target_pose,\n",
    "                    global_orient=target_orient, transl=target_transl\n",
    "                )\n",
    "\n",
    "                target_dict = {'joints': tgt_out.joints, 'vertices': tgt_out.vertices}\n",
    "                _, loss = criterion(\n",
    "                    pred_out, target_dict,\n",
    "                    pred_params=(betas, body_pose, global_orient, transl)\n",
    "                )\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "                mpjpe = compute_mpjpe(pred_out.joints, tgt_out.joints)\n",
    "                mpjpe_list.append(mpjpe)\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_loader)\n",
    "        val_mpjpe = np.mean(mpjpe_list)\n",
    "\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_mpjpe'].append(val_mpjpe)\n",
    "\n",
    "        if epoch <= warmup_epochs:\n",
    "            pass  # Don't step scheduler during warmup\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if val_mpjpe < best_mpjpe:\n",
    "            best_mpjpe = val_mpjpe\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1 or wait == 0:\n",
    "            cur_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'  Epoch {epoch:3d}  train_loss={epoch_loss:.4f}  '\n",
    "                  f'val_loss={val_loss:.4f}  val_MPJPE={val_mpjpe:.1f}mm  '\n",
    "                  f'lr={cur_lr:.1e}  {\"*best\" if wait==0 else \"\"}')\n",
    "\n",
    "        if wait >= patience:\n",
    "            print(f'  Early stopping at epoch {epoch} (patience={patience})')\n",
    "            break\n",
    "\n",
    "    # Restore best weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt_name = config['model']['type'] + '_best.pth'\n",
    "    ckpt_path = CHECKPOINT_DIR / ckpt_name\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'best_mpjpe': best_mpjpe,\n",
    "    }, ckpt_path)\n",
    "    print(f'  Saved best model to {ckpt_path}  (MPJPE={best_mpjpe:.1f}mm)')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train All 5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = {}\n",
    "histories = {}\n",
    "\n",
    "for name, cfg in MODEL_CONFIGS.items():\n",
    "    model_type = cfg['model']['type']\n",
    "    is_rnn = model_type in ('cnn_bigru', 'cnn_bilstm')\n",
    "    is_transformer = model_type == 'cnn_transformer'\n",
    "\n",
    "    model, hist = train_model(\n",
    "        model_name=name,\n",
    "        config=cfg,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        smpl_layer=smpl_layer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LR,\n",
    "        patience=PATIENCE,\n",
    "        grad_clip=GRAD_CLIP if is_rnn else None,\n",
    "        warmup_epochs=WARMUP_EPOCHS if is_transformer else 0,\n",
    "    )\n",
    "\n",
    "    trained_models[name] = model\n",
    "    histories[name] = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot loss curves ----\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for name, hist in histories.items():\n",
    "    axes[0].plot(hist['train_loss'], label=name)\n",
    "    axes[1].plot(hist['val_loss'], label=name)\n",
    "    axes[2].plot(hist['val_mpjpe'], label=name)\n",
    "\n",
    "axes[0].set_title('Train Loss'); axes[0].set_xlabel('Epoch')\n",
    "axes[1].set_title('Val Loss');   axes[1].set_xlabel('Epoch')\n",
    "axes[2].set_title('Val MPJPE (mm)'); axes[2].set_xlabel('Epoch')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROJECT_ROOT / 'output' / 'loss_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, smpl_layer, device):\n",
    "    \"\"\"Compute all metrics on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    mpjpe_all, pa_mpjpe_all, ve_all, ble_all = [], [], [], []\n",
    "    timings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pressure = batch['pressure'].to(device)\n",
    "            target_betas = batch['betas'].to(device)\n",
    "            target_pose = batch['body_pose'].to(device)\n",
    "            target_orient = batch['global_orient'].to(device)\n",
    "            target_transl = batch['transl'].to(device)\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            betas, body_pose, global_orient, transl = model(pressure)\n",
    "            elapsed = (time.perf_counter() - t0) * 1000 / pressure.shape[0]\n",
    "            timings.append(elapsed)\n",
    "\n",
    "            bs = pressure.shape[0]\n",
    "            smpl_layer.batch_size = bs\n",
    "\n",
    "            pred_out = smpl_layer(\n",
    "                betas=betas, body_pose=body_pose,\n",
    "                global_orient=global_orient, transl=transl)\n",
    "            tgt_out = smpl_layer(\n",
    "                betas=target_betas, body_pose=target_pose,\n",
    "                global_orient=target_orient, transl=target_transl)\n",
    "\n",
    "            mpjpe_all.append(compute_mpjpe(pred_out.joints, tgt_out.joints))\n",
    "            pa_mpjpe_all.append(compute_pa_mpjpe(pred_out.joints, tgt_out.joints))\n",
    "            ve_all.append(compute_vertex_error(pred_out.vertices, tgt_out.vertices))\n",
    "            ble_all.append(compute_bone_length_error(pred_out.joints, tgt_out.joints))\n",
    "\n",
    "    return {\n",
    "        'MPJPE (mm)': np.mean(mpjpe_all),\n",
    "        'PA-MPJPE (mm)': np.mean(pa_mpjpe_all),\n",
    "        'Vertex Err (mm)': np.mean(ve_all),\n",
    "        'Bone Len Err (mm)': np.mean(ble_all),\n",
    "        'Inference (ms)': np.mean(timings),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rows = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    metrics = evaluate_model(model, val_loader, smpl_layer, device)\n",
    "    n_params = count_params(model)\n",
    "    row = {'Model': name, 'Params (M)': f'{n_params/1e6:.2f}'}\n",
    "    row.update({k: f'{v:.1f}' for k, v in metrics.items()})\n",
    "    results_rows.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "print('\\n' + '=' * 90)\n",
    "print('MODEL COMPARISON')\n",
    "print('=' * 90)\n",
    "print(results_df.to_string(index=False))\n",
    "print('=' * 90)\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = PROJECT_ROOT / 'output'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(output_dir / 'model_comparison.csv', index=False)\n",
    "print(f'\\nSaved to {output_dir / \"model_comparison.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Showcase Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a prediction from the best model\n",
    "best_name = results_df.iloc[results_df['MPJPE (mm)'].astype(float).idxmin()]['Model']\n",
    "best_model = trained_models[best_name]\n",
    "print(f'Best model: {best_name}')\n",
    "\n",
    "# Get a validation sample\n",
    "sample = val_ds[0]\n",
    "pressure = sample['pressure'].unsqueeze(0).to(device)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    betas, body_pose, global_orient, transl = best_model(pressure)\n",
    "    smpl_layer.batch_size = 1\n",
    "    pred_out = smpl_layer(\n",
    "        betas=betas, body_pose=body_pose,\n",
    "        global_orient=global_orient, transl=transl\n",
    "    )\n",
    "    # Ground truth\n",
    "    tgt_out = smpl_layer(\n",
    "        betas=sample['betas'].unsqueeze(0).to(device),\n",
    "        body_pose=sample['body_pose'].unsqueeze(0).to(device),\n",
    "        global_orient=sample['global_orient'].unsqueeze(0).to(device),\n",
    "        transl=sample['transl'].unsqueeze(0).to(device),\n",
    "    )\n",
    "\n",
    "pred_joints = pred_out.joints[0].cpu().numpy()\n",
    "gt_joints = tgt_out.joints[0].cpu().numpy()\n",
    "\n",
    "# Plot predicted vs GT joints (3D scatter)\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(gt_joints[:24, 0], gt_joints[:24, 2], gt_joints[:24, 1], c='blue', s=30)\n",
    "ax1.set_title('Ground Truth Joints')\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(pred_joints[:24, 0], pred_joints[:24, 2], pred_joints[:24, 1], c='red', s=30)\n",
    "ax2.set_title('Predicted Joints')\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Z'); ax.set_zlabel('Y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mpjpe_val = compute_mpjpe(pred_out.joints, tgt_out.joints)\n",
    "print(f'Sample MPJPE: {mpjpe_val:.1f} mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training and evaluation complete!')\n",
    "print(f'\\nResults saved to: {output_dir}')\n",
    "print(f'Checkpoints saved to: {CHECKPOINT_DIR}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}